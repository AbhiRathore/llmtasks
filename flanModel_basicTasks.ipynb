{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiRathore/llmtasks/blob/main/flanModel_basicTasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l4Nv0pLtcAw",
        "outputId": "7d6bc248-2794-4c8c-c902-281745f4260f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.2\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### mounting gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5XFuX3XDtmU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2595bcf-7bbc-47ad-8dc7-7f5be4d46c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r  /content/drive/MyDrive/llmflan_Tasks/ /content/llmflan_Tasks/"
      ],
      "metadata": {
        "id": "M5Az2MdMnbPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --disable-pip-version-check --quiet -r /content/llmflan_Tasks/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnWW5ixZnbMa",
        "outputId": "98194d4c-c1c6-4bae-8ab9-695ff9446a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torchdata==0.7.0, but you have torchdata 0.5.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNx8lZ-ctkt_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below mentioned yml file can be used to add more user configuration such as sample prompts, target language etc and hence no hardcoding can be maintained to simplify the model deployment and usage"
      ],
      "metadata": {
        "id": "AgwXnvq-G8c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict2use = {}\n",
        "with open(\"/content/llmflan_Tasks/basicTasks/config.yml\", \"r\") as stream:\n",
        "    try:\n",
        "        #print(yaml.safe_load(stream))\n",
        "        dict2use = yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)"
      ],
      "metadata": {
        "id": "ABCvwHjUr89k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if len(dict2use) > 0:\n",
        "  print(\"good to use\")\n",
        "else:\n",
        "  print(\"empty dictionary, plz check yml file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UePx8KDVsC84",
        "outputId": "58e7198d-1de6-406b-d217-a64264898f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good to use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict2use"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkq28K42r860",
        "outputId": "841f638d-5b76-4c76-f80f-ee61af934416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'task': 'Translate',\n",
              " 'Translate': {'baselang': 'English', 'targetlang': 'German'}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use a pre-trained google/flan-t5-small as the model."
      ],
      "metadata": {
        "id": "__y-WZN5vY_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n"
      ],
      "metadata": {
        "id": "cic7cb0it9id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7tIKitLuYsB"
      },
      "outputs": [],
      "source": [
        "set_seed(27122023)\n",
        "model_name='google/flan-t5-small'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkutJ8_RuYpd"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify if the summarization task works."
      ],
      "metadata": {
        "id": "ka8HDXopvbsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue = 'A dollar is a unit of money used in many countries, including the United States, Australia, Canada, and New Zealand. It is represented by the symbol $ and is divided into 100 smaller units called cents'"
      ],
      "metadata": {
        "id": "nsDaYoeKeRUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5yDQW-9uivd",
        "outputId": "9de58075-1179-4986-dd38-8182056c96a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summarize the following conversation by understanding the context. \n",
            "\n",
            "A dollar is a unit of money used in many countries, including the United States, Australia, Canada, and New Zealand. It is represented by the symbol $ and is divided into 100 smaller units called cents\n",
            "\n",
            "Summary:\n",
            " <pad> A dollar is a small unit of money.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize the following conversation by understanding the context.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=10,temperature = .8,\n",
        "            do_sample=True,\n",
        "            top_k=100,\n",
        "            top_p=0.7,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )[0],\n",
        "        skip_special_tokens=False\n",
        "    )\n",
        "print(prompt,output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify if English to French translation task works."
      ],
      "metadata": {
        "id": "fMxDARw4veoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targetlang = dict2use['Translate']['targetlang']"
      ],
      "metadata": {
        "id": "aJ1LvDxBtM9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = f\"translate English to {targetlang}: How old are you?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYPEzlE7qGqX",
        "outputId": "9228b18f-f2e8-454a-93ce-5bb6d8ed933a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> Wie ich er bitten?</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify if the Q&A task works"
      ],
      "metadata": {
        "id": "jKrlpU40vlHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jV4nnXHuYk9",
        "outputId": "a53583c2-e022-4ba1-9875-b52b03ffa401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> John McCain</s>\n"
          ]
        }
      ],
      "source": [
        "input_text = \"answer the following question : who is the president of USA?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Programma'cally print the names of all the model layers and their dimensions."
      ],
      "metadata": {
        "id": "NX8Y0509v-Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about each layer and its dimensions\n",
        "for layer_num, (name, params) in enumerate(model.named_parameters()):\n",
        "    print(f\"Layer {layer_num + 1}: {name}, Size: {params.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am1Ii0c734jK",
        "outputId": "a2202573-6989-4d4b-b768-4503ccd39b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: shared.weight, Size: torch.Size([32128, 512])\n",
            "Layer 2: encoder.block.0.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 3: encoder.block.0.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 4: encoder.block.0.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 5: encoder.block.0.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 6: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Size: torch.Size([32, 6])\n",
            "Layer 7: encoder.block.0.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 8: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 9: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 10: encoder.block.0.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 11: encoder.block.0.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 12: encoder.block.1.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 13: encoder.block.1.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 14: encoder.block.1.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 15: encoder.block.1.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 16: encoder.block.1.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 17: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 18: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 19: encoder.block.1.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 20: encoder.block.1.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 21: encoder.block.2.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 22: encoder.block.2.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 23: encoder.block.2.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 24: encoder.block.2.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 25: encoder.block.2.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 26: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 27: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 28: encoder.block.2.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 29: encoder.block.2.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 30: encoder.block.3.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 31: encoder.block.3.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 32: encoder.block.3.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 33: encoder.block.3.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 34: encoder.block.3.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 35: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 36: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 37: encoder.block.3.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 38: encoder.block.3.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 39: encoder.block.4.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 40: encoder.block.4.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 41: encoder.block.4.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 42: encoder.block.4.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 43: encoder.block.4.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 44: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 45: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 46: encoder.block.4.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 47: encoder.block.4.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 48: encoder.block.5.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 49: encoder.block.5.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 50: encoder.block.5.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 51: encoder.block.5.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 52: encoder.block.5.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 53: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 54: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 55: encoder.block.5.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 56: encoder.block.5.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 57: encoder.block.6.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 58: encoder.block.6.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 59: encoder.block.6.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 60: encoder.block.6.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 61: encoder.block.6.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 62: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 63: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 64: encoder.block.6.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 65: encoder.block.6.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 66: encoder.block.7.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 67: encoder.block.7.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 68: encoder.block.7.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 69: encoder.block.7.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 70: encoder.block.7.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 71: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 72: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 73: encoder.block.7.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 74: encoder.block.7.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 75: encoder.final_layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 76: decoder.block.0.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 77: decoder.block.0.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 78: decoder.block.0.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 79: decoder.block.0.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 80: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Size: torch.Size([32, 6])\n",
            "Layer 81: decoder.block.0.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 82: decoder.block.0.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 83: decoder.block.0.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 84: decoder.block.0.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 85: decoder.block.0.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 86: decoder.block.0.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 87: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 88: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 89: decoder.block.0.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 90: decoder.block.0.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 91: decoder.block.1.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 92: decoder.block.1.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 93: decoder.block.1.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 94: decoder.block.1.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 95: decoder.block.1.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 96: decoder.block.1.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 97: decoder.block.1.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 98: decoder.block.1.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 99: decoder.block.1.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 100: decoder.block.1.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 101: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 102: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 103: decoder.block.1.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 104: decoder.block.1.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 105: decoder.block.2.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 106: decoder.block.2.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 107: decoder.block.2.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 108: decoder.block.2.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 109: decoder.block.2.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 110: decoder.block.2.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 111: decoder.block.2.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 112: decoder.block.2.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 113: decoder.block.2.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 114: decoder.block.2.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 115: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 116: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 117: decoder.block.2.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 118: decoder.block.2.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 119: decoder.block.3.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 120: decoder.block.3.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 121: decoder.block.3.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 122: decoder.block.3.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 123: decoder.block.3.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 124: decoder.block.3.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 125: decoder.block.3.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 126: decoder.block.3.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 127: decoder.block.3.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 128: decoder.block.3.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 129: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 130: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 131: decoder.block.3.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 132: decoder.block.3.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 133: decoder.block.4.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 134: decoder.block.4.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 135: decoder.block.4.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 136: decoder.block.4.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 137: decoder.block.4.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 138: decoder.block.4.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 139: decoder.block.4.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 140: decoder.block.4.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 141: decoder.block.4.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 142: decoder.block.4.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 143: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 144: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 145: decoder.block.4.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 146: decoder.block.4.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 147: decoder.block.5.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 148: decoder.block.5.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 149: decoder.block.5.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 150: decoder.block.5.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 151: decoder.block.5.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 152: decoder.block.5.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 153: decoder.block.5.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 154: decoder.block.5.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 155: decoder.block.5.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 156: decoder.block.5.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 157: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 158: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 159: decoder.block.5.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 160: decoder.block.5.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 161: decoder.block.6.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 162: decoder.block.6.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 163: decoder.block.6.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 164: decoder.block.6.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 165: decoder.block.6.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 166: decoder.block.6.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 167: decoder.block.6.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 168: decoder.block.6.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 169: decoder.block.6.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 170: decoder.block.6.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 171: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 172: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 173: decoder.block.6.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 174: decoder.block.6.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 175: decoder.block.7.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 176: decoder.block.7.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 177: decoder.block.7.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 178: decoder.block.7.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 179: decoder.block.7.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 180: decoder.block.7.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 181: decoder.block.7.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 182: decoder.block.7.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 183: decoder.block.7.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 184: decoder.block.7.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 185: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 186: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 187: decoder.block.7.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 188: decoder.block.7.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 189: decoder.final_layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 190: lm_head.weight, Size: torch.Size([32128, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros"
      ],
      "metadata": {
        "id": "qdanTr499R0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the parameters of the final layer\n",
        "# Print information about each layer and its dimensions\n",
        "new_tensor = 0  # Your tensor\n",
        "\n",
        "for layer_num, (name, params) in enumerate(model.named_parameters()):\n",
        "    if name == \"decoder.final_layer_norm.weight\":\n",
        "        print(f\"Layer {layer_num + 1}: {name}, Size: {params.size()}\")\n",
        "        with torch.no_grad():\n",
        "          for param in params:\n",
        "              param.copy_(new_tensor)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDzoynz3wbE1",
        "outputId": "85310248-9071-4580-c0c8-a491f40f86cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 189: decoder.final_layer_norm.weight, Size: torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer_num, (name, params) in enumerate(model.named_parameters()):\n",
        "    if name == \"decoder.final_layer_norm.weight\":\n",
        "        print(f\"Layer {layer_num + 1}: {name}, param: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8_awey6v44U",
        "outputId": "31010869-3a01-4dd9-dfd4-52569b538545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 189: decoder.final_layer_norm.weight, param: Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify if the Q&A task works aWer reseXng the weights of the above layer"
      ],
      "metadata": {
        "id": "7HOBoYUi9Ygr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"answer the following question in detail by applying a prefix of question: who is the president of USA?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKMm3Zw-5tgU",
        "outputId": "3143e40f-86ff-4f51-b275-25ff80d5ce99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA is not working"
      ],
      "metadata": {
        "id": "pv0WGwUx9aQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension\n"
      ],
      "metadata": {
        "id": "5vKqqlmB9e4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(27122023)\n",
        "\n",
        "model2adjust = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Syi9kCT_HSeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about each layer and its dimensions\n",
        "for layer_num, (name, params) in enumerate(model2adjust.named_parameters()):\n",
        "    print(f\"Layer {layer_num + 1}: {name}, Size: {params.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzrmEerPYkdJ",
        "outputId": "67b41c86-24bf-4b6f-8160-0695a75c9a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: shared.weight, Size: torch.Size([32128, 512])\n",
            "Layer 2: encoder.block.0.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 3: encoder.block.0.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 4: encoder.block.0.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 5: encoder.block.0.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 6: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Size: torch.Size([32, 6])\n",
            "Layer 7: encoder.block.0.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 8: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 9: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 10: encoder.block.0.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 11: encoder.block.0.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 12: encoder.block.1.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 13: encoder.block.1.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 14: encoder.block.1.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 15: encoder.block.1.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 16: encoder.block.1.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 17: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 18: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 19: encoder.block.1.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 20: encoder.block.1.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 21: encoder.block.2.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 22: encoder.block.2.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 23: encoder.block.2.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 24: encoder.block.2.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 25: encoder.block.2.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 26: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 27: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 28: encoder.block.2.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 29: encoder.block.2.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 30: encoder.block.3.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 31: encoder.block.3.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 32: encoder.block.3.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 33: encoder.block.3.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 34: encoder.block.3.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 35: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 36: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 37: encoder.block.3.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 38: encoder.block.3.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 39: encoder.block.4.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 40: encoder.block.4.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 41: encoder.block.4.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 42: encoder.block.4.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 43: encoder.block.4.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 44: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 45: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 46: encoder.block.4.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 47: encoder.block.4.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 48: encoder.block.5.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 49: encoder.block.5.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 50: encoder.block.5.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 51: encoder.block.5.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 52: encoder.block.5.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 53: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 54: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 55: encoder.block.5.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 56: encoder.block.5.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 57: encoder.block.6.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 58: encoder.block.6.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 59: encoder.block.6.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 60: encoder.block.6.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 61: encoder.block.6.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 62: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 63: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 64: encoder.block.6.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 65: encoder.block.6.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 66: encoder.block.7.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 67: encoder.block.7.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 68: encoder.block.7.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 69: encoder.block.7.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 70: encoder.block.7.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 71: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 72: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 73: encoder.block.7.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 74: encoder.block.7.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 75: encoder.final_layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 76: decoder.block.0.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 77: decoder.block.0.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 78: decoder.block.0.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 79: decoder.block.0.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 80: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Size: torch.Size([32, 6])\n",
            "Layer 81: decoder.block.0.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 82: decoder.block.0.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 83: decoder.block.0.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 84: decoder.block.0.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 85: decoder.block.0.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 86: decoder.block.0.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 87: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 88: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 89: decoder.block.0.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 90: decoder.block.0.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 91: decoder.block.1.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 92: decoder.block.1.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 93: decoder.block.1.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 94: decoder.block.1.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 95: decoder.block.1.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 96: decoder.block.1.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 97: decoder.block.1.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 98: decoder.block.1.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 99: decoder.block.1.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 100: decoder.block.1.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 101: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 102: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 103: decoder.block.1.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 104: decoder.block.1.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 105: decoder.block.2.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 106: decoder.block.2.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 107: decoder.block.2.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 108: decoder.block.2.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 109: decoder.block.2.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 110: decoder.block.2.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 111: decoder.block.2.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 112: decoder.block.2.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 113: decoder.block.2.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 114: decoder.block.2.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 115: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 116: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 117: decoder.block.2.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 118: decoder.block.2.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 119: decoder.block.3.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 120: decoder.block.3.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 121: decoder.block.3.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 122: decoder.block.3.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 123: decoder.block.3.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 124: decoder.block.3.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 125: decoder.block.3.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 126: decoder.block.3.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 127: decoder.block.3.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 128: decoder.block.3.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 129: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 130: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 131: decoder.block.3.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 132: decoder.block.3.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 133: decoder.block.4.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 134: decoder.block.4.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 135: decoder.block.4.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 136: decoder.block.4.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 137: decoder.block.4.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 138: decoder.block.4.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 139: decoder.block.4.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 140: decoder.block.4.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 141: decoder.block.4.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 142: decoder.block.4.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 143: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 144: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 145: decoder.block.4.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 146: decoder.block.4.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 147: decoder.block.5.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 148: decoder.block.5.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 149: decoder.block.5.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 150: decoder.block.5.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 151: decoder.block.5.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 152: decoder.block.5.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 153: decoder.block.5.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 154: decoder.block.5.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 155: decoder.block.5.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 156: decoder.block.5.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 157: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 158: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 159: decoder.block.5.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 160: decoder.block.5.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 161: decoder.block.6.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 162: decoder.block.6.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 163: decoder.block.6.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 164: decoder.block.6.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 165: decoder.block.6.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 166: decoder.block.6.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 167: decoder.block.6.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 168: decoder.block.6.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 169: decoder.block.6.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 170: decoder.block.6.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 171: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 172: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 173: decoder.block.6.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 174: decoder.block.6.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 175: decoder.block.7.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 176: decoder.block.7.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 177: decoder.block.7.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 178: decoder.block.7.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 179: decoder.block.7.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 180: decoder.block.7.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 181: decoder.block.7.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 182: decoder.block.7.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 183: decoder.block.7.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 184: decoder.block.7.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 185: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 186: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 187: decoder.block.7.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 188: decoder.block.7.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 189: decoder.final_layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 190: lm_head.weight, Size: torch.Size([32128, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model2adjust = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "config = model2adjust.config\n",
        "# Define a low-dimensional layer\n",
        "low_dim_size = 128\n",
        "low_dim_layer = nn.Linear(config.d_model, low_dim_size)\n",
        "\n",
        "# Replace the last decoder with the low-dimensional layer\n",
        "model2adjust.decoder.final_layer_norm = low_dim_layer\n",
        "\n",
        "# Adjust the last layer with the low-dimensional layer\n",
        "outputDim = model2adjust.lm_head.weight.data.shape[0]\n",
        "model2adjust.lm_head = nn.Linear(low_dim_size, outputDim)\n",
        "for layer_num, (name, params) in enumerate(model2adjust.named_parameters()):\n",
        "    print(f\"Layer {layer_num + 1}: {name}, Size: {params.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQaCmNzgHfIi",
        "outputId": "2d3b4877-77ba-4bd0-a928-34d4fd3dddd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: shared.weight, Size: torch.Size([32128, 512])\n",
            "Layer 2: encoder.block.0.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 3: encoder.block.0.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 4: encoder.block.0.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 5: encoder.block.0.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 6: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Size: torch.Size([32, 6])\n",
            "Layer 7: encoder.block.0.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 8: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 9: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 10: encoder.block.0.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 11: encoder.block.0.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 12: encoder.block.1.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 13: encoder.block.1.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 14: encoder.block.1.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 15: encoder.block.1.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 16: encoder.block.1.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 17: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 18: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 19: encoder.block.1.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 20: encoder.block.1.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 21: encoder.block.2.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 22: encoder.block.2.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 23: encoder.block.2.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 24: encoder.block.2.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 25: encoder.block.2.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 26: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 27: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 28: encoder.block.2.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 29: encoder.block.2.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 30: encoder.block.3.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 31: encoder.block.3.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 32: encoder.block.3.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 33: encoder.block.3.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 34: encoder.block.3.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 35: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 36: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 37: encoder.block.3.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 38: encoder.block.3.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 39: encoder.block.4.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 40: encoder.block.4.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 41: encoder.block.4.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 42: encoder.block.4.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 43: encoder.block.4.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 44: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 45: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 46: encoder.block.4.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 47: encoder.block.4.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 48: encoder.block.5.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 49: encoder.block.5.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 50: encoder.block.5.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 51: encoder.block.5.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 52: encoder.block.5.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 53: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 54: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 55: encoder.block.5.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 56: encoder.block.5.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 57: encoder.block.6.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 58: encoder.block.6.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 59: encoder.block.6.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 60: encoder.block.6.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 61: encoder.block.6.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 62: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 63: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 64: encoder.block.6.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 65: encoder.block.6.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 66: encoder.block.7.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 67: encoder.block.7.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 68: encoder.block.7.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 69: encoder.block.7.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 70: encoder.block.7.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 71: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 72: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 73: encoder.block.7.layer.1.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 74: encoder.block.7.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 75: encoder.final_layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 76: decoder.block.0.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 77: decoder.block.0.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 78: decoder.block.0.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 79: decoder.block.0.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 80: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Size: torch.Size([32, 6])\n",
            "Layer 81: decoder.block.0.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 82: decoder.block.0.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 83: decoder.block.0.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 84: decoder.block.0.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 85: decoder.block.0.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 86: decoder.block.0.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 87: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 88: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 89: decoder.block.0.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 90: decoder.block.0.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 91: decoder.block.1.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 92: decoder.block.1.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 93: decoder.block.1.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 94: decoder.block.1.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 95: decoder.block.1.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 96: decoder.block.1.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 97: decoder.block.1.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 98: decoder.block.1.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 99: decoder.block.1.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 100: decoder.block.1.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 101: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 102: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 103: decoder.block.1.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 104: decoder.block.1.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 105: decoder.block.2.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 106: decoder.block.2.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 107: decoder.block.2.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 108: decoder.block.2.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 109: decoder.block.2.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 110: decoder.block.2.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 111: decoder.block.2.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 112: decoder.block.2.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 113: decoder.block.2.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 114: decoder.block.2.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 115: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 116: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 117: decoder.block.2.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 118: decoder.block.2.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 119: decoder.block.3.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 120: decoder.block.3.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 121: decoder.block.3.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 122: decoder.block.3.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 123: decoder.block.3.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 124: decoder.block.3.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 125: decoder.block.3.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 126: decoder.block.3.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 127: decoder.block.3.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 128: decoder.block.3.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 129: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 130: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 131: decoder.block.3.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 132: decoder.block.3.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 133: decoder.block.4.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 134: decoder.block.4.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 135: decoder.block.4.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 136: decoder.block.4.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 137: decoder.block.4.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 138: decoder.block.4.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 139: decoder.block.4.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 140: decoder.block.4.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 141: decoder.block.4.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 142: decoder.block.4.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 143: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 144: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 145: decoder.block.4.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 146: decoder.block.4.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 147: decoder.block.5.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 148: decoder.block.5.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 149: decoder.block.5.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 150: decoder.block.5.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 151: decoder.block.5.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 152: decoder.block.5.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 153: decoder.block.5.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 154: decoder.block.5.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 155: decoder.block.5.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 156: decoder.block.5.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 157: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 158: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 159: decoder.block.5.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 160: decoder.block.5.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 161: decoder.block.6.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 162: decoder.block.6.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 163: decoder.block.6.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 164: decoder.block.6.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 165: decoder.block.6.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 166: decoder.block.6.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 167: decoder.block.6.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 168: decoder.block.6.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 169: decoder.block.6.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 170: decoder.block.6.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 171: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 172: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 173: decoder.block.6.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 174: decoder.block.6.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 175: decoder.block.7.layer.0.SelfAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 176: decoder.block.7.layer.0.SelfAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 177: decoder.block.7.layer.0.SelfAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 178: decoder.block.7.layer.0.SelfAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 179: decoder.block.7.layer.0.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 180: decoder.block.7.layer.1.EncDecAttention.q.weight, Size: torch.Size([384, 512])\n",
            "Layer 181: decoder.block.7.layer.1.EncDecAttention.k.weight, Size: torch.Size([384, 512])\n",
            "Layer 182: decoder.block.7.layer.1.EncDecAttention.v.weight, Size: torch.Size([384, 512])\n",
            "Layer 183: decoder.block.7.layer.1.EncDecAttention.o.weight, Size: torch.Size([512, 384])\n",
            "Layer 184: decoder.block.7.layer.1.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 185: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Size: torch.Size([1024, 512])\n",
            "Layer 186: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Size: torch.Size([1024, 512])\n",
            "Layer 187: decoder.block.7.layer.2.DenseReluDense.wo.weight, Size: torch.Size([512, 1024])\n",
            "Layer 188: decoder.block.7.layer.2.layer_norm.weight, Size: torch.Size([512])\n",
            "Layer 189: decoder.final_layer_norm.weight, Size: torch.Size([128, 512])\n",
            "Layer 190: decoder.final_layer_norm.bias, Size: torch.Size([128])\n",
            "Layer 191: lm_head.weight, Size: torch.Size([32128, 128])\n",
            "Layer 192: lm_head.bias, Size: torch.Size([32128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A low dimension (128 size) layer has replayed the last decoder block"
      ],
      "metadata": {
        "id": "Vlky9o6LHj3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the functionality after changing the dimension"
      ],
      "metadata": {
        "id": "CC_t5O4dX-Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "dialogue = \"the apple is a big country but apple is also a fruit and a mobile\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "        model2adjust.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=5,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaMYuY7xVvlo",
        "outputId": "d60410df-b6c2-42e4-d993-4fc32b76b2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "violent într visibility Timişoara rel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is working fine"
      ],
      "metadata": {
        "id": "pFylmqUrYCpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making one function to allow user perform translation, summarization or QnA"
      ],
      "metadata": {
        "id": "w5YrcsuEaBs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-small'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
      ],
      "metadata": {
        "id": "BKG97ioEcL9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class llmFlan:\n",
        "  def __init__(self, model,tokenizer,task,text):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.task = task\n",
        "    self.text = text\n",
        "\n",
        "\n",
        "  def summarization(self):\n",
        "    prompt = f\"\"\"\n",
        "    Summarize the following conversation by understanding the context.\n",
        "\n",
        "    {self.text}\n",
        "\n",
        "    Summary:\n",
        "    \"\"\"\n",
        "    inputs = self.tokenizer(prompt, return_tensors='pt')\n",
        "    output = self.tokenizer.decode(\n",
        "            self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_new_tokens=10,temperature = .9,\n",
        "                do_sample=True,\n",
        "                top_k=30,\n",
        "                top_p=0.9,\n",
        "                num_return_sequences=1\n",
        "            )[0],\n",
        "            skip_special_tokens=False\n",
        "        )\n",
        "    print(prompt,output)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def translation(self,targetlang='German'):\n",
        "    input_text = f\"translate English to {targetlang}: {self.text}\"\n",
        "    input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    outputs = self.model.generate(input_ids)\n",
        "    return self.tokenizer.decode(outputs[0])\n",
        "\n",
        "\n",
        "  def QnA(self):\n",
        "    print(\"please ask a Question\")\n",
        "    input_text = f\"answer the following question : {self.text}\"\n",
        "    input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    outputs = self.model.generate(input_ids)\n",
        "    return self.tokenizer.decode(outputs[0])\n",
        "\n",
        "  def run(self):\n",
        "    if task == 'translation':\n",
        "      return self.translation()\n",
        "    elif task == 'summarization':\n",
        "      return self.summarization()\n",
        "    elif task == 'QnA':\n",
        "      return self.QnA()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mRqdO_zLaAW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = 'summarization'\n",
        "text = 'A dollar is a unit of money used in many countries, including the United States, Australia, Canada, and New Zealand. It is represented by the symbol $ and is divided into 100 smaller units called cents'"
      ],
      "metadata": {
        "id": "wp-xI_3adRdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llmmain = llmFlan(model,tokenizer,text,task)\n",
        "llmmain.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "pBzJ42cMaAUK",
        "outputId": "f018db2b-4615-4774-b672-2feb6aec359a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Summarize the following conversation by understanding the context.  \n",
            "\n",
            "    summarization\n",
            "\n",
            "    Summary:\n",
            "     <pad> W: This is the most common method to track\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> W: This is the most common method to track'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### some Hallucination is there and that needs to be treated"
      ],
      "metadata": {
        "id": "IaJQ8a_2oy2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r llmflan_Tasks/basicTasks/ /content/drive/MyDrive/llmflan_Tasks"
      ],
      "metadata": {
        "id": "39-PPp71bvWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/Colab Notebooks/flanModel_basicTasks.ipynb' /content/drive/MyDrive/llmflan_Tasks/basicTasks"
      ],
      "metadata": {
        "id": "g_7fUucFISTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1tj_VkncTS3P7iN8fB8rOo56nWKlVD9-Q",
      "authorship_tag": "ABX9TyPceSiebD8s9XIBYN59G35P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}